{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Input Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=\"Bpk @ridwankamil43 tidak sebaik dahulu 20 ridwan43@gmail.com pic.twitter.com/DDAFssa232dsa #ridwangembel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "kumpText=[]\n",
    "def preprocessing(text=\"\",\n",
    "                  casefolding=True,\n",
    "                  removeusername=True,\n",
    "                  removenonalphabet=True,\n",
    "                  removestopword=True,\n",
    "                  stemming=True,\n",
    "                  removehashtag=True,\n",
    "                  removeemail=True,\n",
    "                  removeAdr=True):\n",
    "    \n",
    "    if(text!=\"\"):\n",
    "        import re\n",
    "       \n",
    "        if(casefolding):\n",
    "            text = text.lower()\n",
    "            print(\"Case Folding = \"+text)\n",
    "            \n",
    "        if(removeusername):\n",
    "            nama = re.compile(\"(?:^|\\s)[＠ @]{1}([^\\s#<>[\\]|{}]+)\", re.UNICODE)\n",
    "            text = re.sub(nama, ' ', text)\n",
    "            print(\"Remove Username = \"+text)\n",
    "\n",
    "        \n",
    "        if(removeAdr):\n",
    "            com= re.compile(r' \\w+\\.\\w+\\.*\\w*\\D\\w*')\n",
    "            text = re.sub(com, ' ', text)\n",
    "            print(\"Remove Address = \"+text)\n",
    "\n",
    "            \n",
    "        if(removehashtag):\n",
    "            hashtag = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "            text = re.sub(hashtag, ' ', text)\n",
    "            print(\"Remove Hashtag = \"+text)\n",
    " \n",
    "        if(removeemail):\n",
    "            email = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+')\n",
    "            text = re.sub(email, ' ', text) \n",
    "            print(\"Remove Email = \"+text)\n",
    "            \n",
    "        if(removenonalphabet):\n",
    "            import re\n",
    "            text = ' '.join(re.findall(r'\\b[a-z]+?[a-z]+\\b',text))\n",
    "            print(\"Remove non Alphabet = \"+text)\n",
    "    \n",
    "        if(removestopword):\n",
    "            from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "            factory = StopWordRemoverFactory()\n",
    "            Sastrawi_StopWords_id = factory.get_stop_words()\n",
    "            temp = [t for t in re.findall(r'\\b[a-z]+-?[a-z]+\\b',text) if t not in Sastrawi_StopWords_id]\n",
    "            text = ' '.join(temp)\n",
    "            print(\"Remove Stopword = \"+text)\n",
    "            \n",
    "        if(stemming):\n",
    "            from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "            stemmer = StemmerFactory().create_stemmer()\n",
    "            text   = stemmer.stem(text)\n",
    "            print(\"Stemming = \"+text)\n",
    "            \n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mengolah Teks dan Normalisasi Kata</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding = bpk @ridwankamil43 tidak sebaik dahulu 20 ridwan43@gmail.com pic.twitter.com/ddafssa232dsa #ridwangembel\n",
      "Remove Username = bpk  tidak sebaik dahulu 20 ridwan43@gmail.com pic.twitter.com/ddafssa232dsa #ridwangembel\n",
      "Remove Address = bpk  tidak sebaik dahulu 20 ridwan43@gmail.com  #ridwangembel\n",
      "Remove Hashtag = bpk  tidak sebaik dahulu 20 ridwan43@gmail.com  \n",
      "Remove Email = bpk  tidak sebaik dahulu 20    \n",
      "Remove non Alphabet = bpk tidak sebaik dahulu\n",
      "Remove Stopword = bpk tidak sebaik\n",
      "Stemming = bpk tidak baik\n",
      "Normalisasi Kata =   bapak tidak baik  \n"
     ]
    }
   ],
   "source": [
    "bersih=preprocessing(text)\n",
    "\n",
    "file = open(\"kamus.txt\", encoding=\"utf-8\", errors='replace')\n",
    "Teks = file.readlines()\n",
    "Teks_string=' '.join(Teks)\n",
    "fix=eval(Teks_string)\n",
    "\n",
    "\n",
    "t=\" \",bersih,\" \"\n",
    "t_string=' '.join(t)\n",
    "t_jadi=t_string \n",
    "    \n",
    "for slang, formal in fix.items():\n",
    "    t_jadi = t_jadi.replace(slang,formal)\n",
    "print(\"Normalisasi Kata = \"+t_jadi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Negasi</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bapak   buruk jahat zalim\n"
     ]
    }
   ],
   "source": [
    "import nlp_if_lib as nlp\n",
    "import urllib, json\n",
    "import requests\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id_spacy = Indonesian() \n",
    "\n",
    "teksAnt=[]\n",
    "\n",
    "kataStrip=t_jadi.strip()\n",
    "kataToken = nlp_id_spacy(kataStrip)  \n",
    "token_pos = nlp.getPOSTag()\n",
    "    \n",
    "for x in kataToken:\n",
    "    teksAnt.append(x.text)\n",
    "    \n",
    "lenTeksAnt=len(teksAnt)   \n",
    "for y in range(0,lenTeksAnt):\n",
    "    antonim=[]\n",
    "    if(teksAnt[y]==\"tidak\" or teksAnt[y]==\"kurang\" or teksAnt[y]==\"belum\"):\n",
    "        for token in kataToken:\n",
    "            t=token.lemma_\n",
    "            try: \n",
    "                pos=token_pos[token.lemma_]\n",
    "                if(token_pos[teksAnt[y+1]]==\"adj\"):\n",
    "                    url = \"http://kateglo.com/api.php?format=json&phrase=\"+teksAnt[y+1]\n",
    "                    response = requests.get(url)\n",
    "                    data=response.text\n",
    "                    parsed=json.loads(data)\n",
    "                    \n",
    "                    post = parsed[\"kateglo\"][\"relation\"][\"a\"]\n",
    "                    lenPost=len(post)\n",
    "                    for x in range(0,lenPost):\n",
    "                        st=str(x)\n",
    "                        value=post[st]\n",
    "                        if(value['rel_type']=='a'):\n",
    "                            if(value['lex_class']=='adj'):\n",
    "                                ant=value['related_phrase']\n",
    "                                antonim.append(ant)\n",
    "                                teksAnt[y+1]=\"\"\n",
    "                                teksAnt[y]=\"\"\n",
    "\n",
    "            except:\n",
    "                pos='n'\n",
    "            \n",
    "                \n",
    "            \n",
    "    listAnt=len(antonim)\n",
    "    for i in range(0,listAnt):\n",
    "        teksAnt.append(antonim[i])\n",
    "        \n",
    "ubahAnt=' '.join(teksAnt)\n",
    "kataJadi=ubahAnt\n",
    "print(kataJadi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Semantic Expansion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bapak buruk jahat zalim aus bapet bejat busuk cabul cendala gabas hina inferior jahat jelek kasar keji korup kotor lapuk lecak leceh negatif nista odoh rebeh rendah rombeng rusak sergut subal tengik usang bandel bangor bangpak bengal bengis biadab buas busuk curang dengki durjana hina jahanam jahil jalang jelek kejam keji kotor lacur licik nakal pasik rusak sadis sundal tambung tebal hati bengis kejam\n"
     ]
    }
   ],
   "source": [
    "#Menggunakan file kata_dasar.txt\n",
    "import PyPDF2\n",
    "import urllib, json\n",
    "import nlp_if_lib as nlp\n",
    "from spacy.lang.id import Indonesian\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "\n",
    "nlp_id_spacy = Indonesian() \n",
    "kataEx=[]\n",
    "\n",
    "\n",
    "#Teks=kataToken,tweet=kataStrip\n",
    "\n",
    "\n",
    "sinonim=[]\n",
    "teksA=[]\n",
    "kataStrip=kataJadi.strip()\n",
    "kataTokens = nlp_id_spacy(kataStrip)  \n",
    "token_pos = nlp.getPOSTag()\n",
    "    \n",
    "for x in kataTokens:#tokenisasi tweet\n",
    "    teksA.append(x.text)\n",
    "    \n",
    "for tokens in kataTokens:\n",
    "    try:\n",
    "        pos=token_pos[tokens.lemma_]\n",
    "        if(token_pos[tokens.lemma_]==\"adj\"):\n",
    "            kataAdj=tokens.lemma_\n",
    "            \n",
    "            url = \"http://kateglo.com/api.php?format=json&phrase=\"+kataAdj\n",
    "            response = requests.get(url)\n",
    "            data=response.text\n",
    "            parsed=json.loads(data)\n",
    "            \n",
    "            post = parsed[\"kateglo\"][\"all_relation\"]\n",
    "            \n",
    "            lenPost=len(post)\n",
    "            for x in range(0,lenPost):\n",
    "                value=post[x]\n",
    "                if(value['rel_type']=='s'):\n",
    "                    if(value['lex_class']=='adj'):\n",
    "                        syn=value['related_phrase']\n",
    "                        sinonim.append(syn)\n",
    "            \n",
    "    except:\n",
    "        pos='n'\n",
    "             \n",
    "        \n",
    "listSyn=len(sinonim)\n",
    "for z in range(0,listSyn):\n",
    "    teksA.append(sinonim[z])\n",
    "    \n",
    "queryEx=' '.join(teksA)\n",
    "kataEx.append(queryEx)\n",
    "\n",
    "st=kataEx[0].strip()\n",
    "subs=re.sub(' +', ' ',st)\n",
    "kata=subs\n",
    "print(kata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "oke\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "filename='MNBEx.model'\n",
    "\n",
    "file=open('model\\\\'+filename,'rb')\n",
    "bobot_model=pickle.load(file)\n",
    "model=pickle.load(file)\n",
    "\n",
    "print(bobot_model)\n",
    "print(model)\n",
    "\n",
    "print('oke')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pembobotan</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rusak 0.5288520073602871\n",
      "korup 0.2644260036801436\n",
      "hina 0.5288520073602871\n",
      "busuk 0.5288520073602871\n",
      "buruk 0.21333723480763117\n",
      "bapak 0.21333723480763117\n"
     ]
    }
   ],
   "source": [
    "bobot=bobot_model.transform([kata])\n",
    "matrix_bobot=bobot.toarray()\n",
    "x_test=matrix_bobot\n",
    "\n",
    "doc = 0\n",
    "feature_names=bobot_model.get_feature_names()\n",
    "feature_index=bobot[doc,:].nonzero()[1]\n",
    "tfidf_scores=zip(feature_index,[bobot[doc,x] for x in feature_index])\n",
    "\n",
    "for w, s in [(feature_names[i],s) for (i,s) in tfidf_scores]:\n",
    "    print(w,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prediksi</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positif :0.35609357832372884\n",
      "negatif :0.6439064216762707\n"
     ]
    }
   ],
   "source": [
    "predict=model.predict(x_test)\n",
    "prob_n, prob_p=model.predict_proba(x_test)[0]\n",
    "\n",
    "print(\"positif :\"+str(prob_p))\n",
    "print(\"negatif :\"+str(prob_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hasil</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bpk @ridwankamil43 tidak sebaik dahulu 20 ridwan43@gmail.com pic.twitter.com/DDAFssa232dsa #ridwangembel\n",
      "negatif\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "if(predict[0]==1):\n",
    "    print(\"positif\")\n",
    "else:\n",
    "    print(\"negatif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
